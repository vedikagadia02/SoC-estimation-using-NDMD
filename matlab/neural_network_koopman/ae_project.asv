
miniBatchSize = 128;
[training_input_data, training_target_data] = dataLoader(); 
% arrays of shape 750400,1,2 and 750400,30,2
training_input_data = training_input_data(1:256, 1, 1:2);
[mbq_input, mbq_target] = createMiniBatch(training_input_data, training_target_data, miniBatchSize);
[netE, netK, netD] = define_nn_arch();

encoder_lr = 0.0005;
kMatrix_lr = 0.001;
encoderEpochs = 5;
koopmanEpochs = 20;
s = 30;
a1 = 1;
a2 = 50;
a3 = 10;
a4 = 1e-6;
numObservationsTrain = size(training_input_data,1);
numIterationsPerEpoch = ceil(numObservationsTrain / miniBatchSize);
encoderNumIterations = encoderEpochs * numIterationsPerEpoch;
koopmanNumIterations = koopmanEpochs * numIterationsPerEpoch;
trailingAvgE = [];
trailingAvgSqE = [];
trailingAvgK = [];
trailingAvgSqK = [];
trailingAvgD = [];
trailingAvgSqD = [];

monitor = trainingProgressMonitor( ...
    Metrics="Loss", ...
    Info="Epoch", ...
    XLabel="Iteration" ...
);

epoch = 0;
iteration = 0;

% Only autoencoder
while epoch < encoderEpochs && ~monitor.Stop
    epoch = epoch + 1;
    % How to shuffle both the input and target mini batch queues together? 
    % Or create a joint datastore of training_input_data and training_target_data
    % Temporary solution : don't shuffle the mini batch
    shuffle(mbq_input);

    % Loop over mini-batches.
    while hasdata(mbq_input) && ~monitor.Stop
        iteration = iteration + 1;

        % Read mini-batch of data.
        X = next(mbq_input);                                    % dlarray of shape 2,1,128
        % Evaluate loss and gradients.
        [loss,gradientsE,gradientsD] = dlfeval(@autoencoder,netE,netD,X, miniBatchSize);

        % Update learnable parameters.
        [netE,trailingAvgE,trailingAvgSqE] = adamupdate(netE, ...
            gradientsE,trailingAvgE,trailingAvgSqE,iteration,encoder_lr);
        [netD, trailingAvgD, trailingAvgSqD] = adamupdate(netD, ...
            gradientsD,trailingAvgD,trailingAvgSqD,iteration,encoder_lr);

        % Update the training progress monitor. 
        recordMetrics(monitor,iteration,Loss=loss);
        updateInfo(monitor,Epoch=epoch + " of " + encoderEpochs);
        monitor.Progress = 100*iteration/encoderNumIterations;
    end
end

epoch = 0;
iteration = 0;

% Encoder -> koopman -> decoder
while epoch < koopmanEpochs && ~monitor.Stop
    epoch = epoch + 1;
    % How to shuffle both the input and target mini batch queues together? 
    % Or create a joint datastore of training_input_data and training_target_data
    % Temporary solution : don't shuffle the mini batch
    % shuffle(mbq_input);
    % shuffle(mbq_target);

    % Loop over mini-batches.
    while hasdata(mbq_input) && ~monitor.Stop
        iteration = iteration + 1;

        % Read mini-batch of data.
        X = next(mbq_input);                                    % dlarray of shape 2,1,128                          
        target = next(mbq_target);                              % dlarray of shape 2,30,128

        % Evaluate loss and gradients.
        [loss,gradientsE,gradientsK,gradientsD] = dlfeval(@koopmanModel,netE,netK,netD,X, target, miniBatchSize, s, a1, a2, a3, a4);

        % Update learnable parameters.
        [netE,trailingAvgE,trailingAvgSqE] = adamupdate(netE, ...
            gradientsE,trailingAvgE,trailingAvgSqE,iteration,encoder_lr);
        [netK, trailingAvgK, trailingAvgSqK] = adamupdate(netK, ...
            gradientsK,trailingAvgK,trailingAvgSqK,iteration,kMatrix_lr);
        [netD, trailingAvgD, trailingAvgSqD] = adamupdate(netD, ...
            gradientsD,trailingAvgD,trailingAvgSqD,iteration,encoder_lr);

        % Update the training progress monitor. 
        recordMetrics(monitor,iteration,Loss=loss);
        updateInfo(monitor,Epoch=epoch + " of " + koopmanEpochs);
        monitor.Progress = 100*iteration/koopmanNumIterations;
    end
end

function X = preprocessMiniBatch(dataX)
    X = cat(1,dataX{:});
end
 
function [mbq_input, mbq_target] = createMiniBatch(training_input_data, training_target_data, miniBatchSize)
    dsInput = arrayDatastore(training_input_data,IterationDimension=1);
    numOutputs = 1;
    mbq_input = minibatchqueue(dsInput,numOutputs, ...
        MiniBatchSize = miniBatchSize, ...
        MiniBatchFcn=@preprocessMiniBatch, ...
        MiniBatchFormat="BCS", ...
        PartialMiniBatch="discard");
    
    dsTarget = arrayDatastore(training_target_data,IterationDimension=1);
    numOutputs = 1;
    mbq_target = minibatchqueue(dsTarget,numOutputs, ...
        MiniBatchSize = miniBatchSize, ...
        MiniBatchFcn=@preprocessMiniBatch, ...
        MiniBatchFormat="BCS", ...
        PartialMiniBatch="discard");
end


function [Y] = encoder(netE, X, miniBatchSize)
    Yout = forward(netE, X);                                    % list of shape 3,128
    Ytemp = reshape(Yout, 3, 1, miniBatchSize);                 % list of shape 3,1,128
    Y = dlarray(Ytemp, 'SCB');                  
end

function [K] = koopman(netK, Y, miniBatchSize)
    Kout = forward(netK, Y);
    Ktemp = reshape(Kout, 3, 1, miniBatchSize);
    K = dlarray(Ktemp, 'SCB');
end

function [Z] = decoder(netD, Y, miniBatchSize)
    Zout = forward(netD, Y);
    Ztemp = reshape(Zout, 2, 1, miniBatchSize);
    Z = dlarray(Ztemp, 'SCB');
end


function [loss,gradientsE,gradientsD] = autoencoder(netE, netD, X, miniBatchSize)
    Y = encoder(netE, X, miniBatchSize);                        % dlarray of shape 3,1,128
    Z = decoder(netD, Y, miniBatchSize);                        % dlarray of shape 2,1,128
    loss = encoderLoss(Z,X);
    [gradientsE,gradientsD] = dlgradient(loss,netE.Learnables,netD.Learnables);
end

function loss = encoderLoss(Z,X)
    reconstructionLoss = mse(Z,X);
    loss = reconstructionLoss;
end

function [loss, gradientsE, gradientsK, gradientsD] = koopmanModel(netE, netK, netD, X, target, miniBatchSize, s, a1, a2, a3, a4)
    encoderOutputT = encoder(netE, X, miniBatchSize);           % dlarray of shape 3,1,128
    outputT = decoder(netD, encoderOutputT, miniBatchSize);     % dlarray of shape 2,1,128
    loss_lin = 0;
    loss_pred = 0;
    loss_rec = encoderLoss(outputT,X);

    for i = 1:1:s
        inputTnext = target(:,i,:);
        encoderOutputTnext = encoder(netE, inputTnext, miniBatchSize);
        outputTnext = decoder(netD, encoderOutputTnext, miniBatchSize);
        koopmanTnext = koopman(netK, encoderOutputT, miniBatchSize);
        recoverTnext = decoder(netD, koopmanTnext, miniBatchSize);

        loss_rec = loss_rec + encoderLoss(inputTnext, outputTnext);
        loss_lin = loss_lin + mse(koopmanTnext, encoderOutputTnext);
        loss_pred = loss_pred + mse(inputTnext, recoverTnext);
    end

    loss_rec = loss_rec/s;
    loss_lin = loss_lin/s;
    loss_pred = loss_pred/s;
    
    loss = a1*loss_rec + a2*loss_lin + a3*loss_pred;
    [gradientsE, gradientsK, gradientsD] = dlgradient(loss, netE.Learnables, netK.Learnables, netD.Learnables);

end
